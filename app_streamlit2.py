import streamlit as st
import requests
import json
import os
import pickle
from streamlit_chat import message  # Ù…ÙƒØªØ¨Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„

# ğŸ”¹ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Ollama Chat",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ğŸ“Œ Ù…Ù„Ù ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
CHAT_HISTORY_FILE = "chat_history.pkl"

# ğŸ”¥ ØªØ­Ø³ÙŠÙ† Ø·Ø±ÙŠÙ‚Ø© ØªØ­Ù…ÙŠÙ„ ÙˆØ­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø¨Ø§Ø³ØªØ¹Ù…Ø§Ù„ pickle
def load_chat_history():
    if os.path.exists(CHAT_HISTORY_FILE):
        try:
            with open(CHAT_HISTORY_FILE, "rb") as file:
                return pickle.load(file)
        except Exception:
            return []
    return []

def save_chat_history():
    with open(CHAT_HISTORY_FILE, "wb") as file:
        pickle.dump(st.session_state.messages, file)

# ğŸ”¥ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
if "messages" not in st.session_state:
    st.session_state.messages = load_chat_history()

# ğŸ”¥ Ø¯Ø§Ù„Ø© Ù„Ø¬Ù„Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù…Ù† Ollama
def get_available_models():
    try:
        response = requests.get('http://localhost:11434/api/tags')
        if response.status_code == 200:
            models = response.json()
            return [model['name'] for model in models['models']]
        return []
    except:
        return []

# ğŸ”¥ ØªØ­Ø³ÙŠÙ† Ø·Ø±ÙŠÙ‚Ø© Ø¥Ø±Ø³Ø§Ù„ ÙˆØ§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ù…Ù† Ollama Ø¨Ø§Ø³ØªØ¹Ù…Ø§Ù„ Streaming
def chat_with_ollama():
    """Send chat history to Ollama and stream responses dynamically."""
    try:
        response = requests.post(
            'http://localhost:11434/api/generate',
            json={
                "model": st.session_state.selected_model,
                "prompt": format_chat_history(),
                "stream": True
            },
            stream=True
        )

        response_text = ""  
        message_placeholder = st.empty()  

        if response.status_code == 200:
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)  
                        chunk = data.get("response", "")  
                        response_text += chunk  

                        # âœ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Øµ Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø¹Ø±Ø¶ ÙƒÙ„ Ø´ÙŠØ¡
                        message_placeholder.markdown(f"**AI:** {response_text}")
                        st.session_state.messages[-1]["content"] = response_text  
                    except json.JSONDecodeError:
                        continue  

        return response_text  
    except Exception as e:
        return f"Error: {str(e)}"

# ğŸ”¥ ØªØ­Ø³ÙŠÙ† Ø·Ø±ÙŠÙ‚Ø© ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù„Ù„Ù€ AI Ø¨Ø§Ø´ ÙŠÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ø²ÙŠØ§Ù†
def format_chat_history():
    """Format chat history for better context."""
    history = "\n".join([f"{'User' if msg['role'] == 'user' else 'AI'}: {msg['content']}" for msg in st.session_state.messages[-5:]])
    return history + "\nAI:"

# ğŸ”¥ Ø¥Ø¶Ø§ÙØ© Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙÙŠ Ø§Ù„Ù€ Sidebar
st.sidebar.title("ğŸ¤– Ollama Chat Settings")
available_models = get_available_models()
if available_models:
    st.session_state.selected_model = st.sidebar.selectbox(
        "Select Model",
        available_models,
        index=0
    )
else:
    st.sidebar.error("No models found. Please make sure Ollama is running.")
    st.session_state.selected_model = None

st.sidebar.markdown("---")
st.sidebar.markdown("""
### How to use:
1. Make sure Ollama is running locally.
2. Select a model from the dropdown.
3. Type your message and press Enter.
4. Wait for the AI to respond.
""")

# ğŸ”¥ ØªØ­Ø³ÙŠÙ† Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨Ø§Ø³ØªØ¹Ù…Ø§Ù„ `streamlit_chat`
st.title("ğŸ¤– Ollama Chat")

# âœ… Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø­Ø³Ù†Ø©
for i, msg in enumerate(st.session_state.messages):
    is_user = msg["role"] == "user"
    message(msg["content"], is_user=is_user, key=str(i))  

# ğŸ”¥ Ø¥Ø¶Ø§ÙØ© Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
if st.session_state.selected_model:
    with st.form(key="chat_form"):
        user_input = st.text_area("Type your message:", key="chat_input", height=100)
        submit_button = st.form_submit_button("Send")
        
        if submit_button and user_input.strip():
            # âœ… Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            st.session_state.messages.append({"role": "user", "content": user_input})
            
            # âœ… Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© AI ÙØ§Ø±ØºØ© Ù„ÙŠØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡Ø§ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ§Ù‹
            st.session_state.messages.append({"role": "assistant", "content": ""})

            # âœ… Ø·Ù„Ø¨ Ø§Ù„Ø±Ø¯ Ù…Ù† Ollama
            with st.spinner("AI is thinking..."):
                ai_response = chat_with_ollama()

            # âœ… Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            save_chat_history()
            
            # âœ… ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
            st.rerun()
else:
    st.warning("Please make sure Ollama is running and select a model to start chatting.")
